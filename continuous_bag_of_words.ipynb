{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Bag of Words (CBOW) Model\n",
    "\n",
    "The CBOW model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). Considering a simple sentence, __\"the quick brown fox jumps over the lazy dog\"__, this can be pairs of __(context_window, target_word)__ where if we consider a context window of size 2, we have examples like __([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy)__ and so on. Thus the model tries to predict the __target_word__ based on the __context_window_words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import vocab_utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the spacy english model is installed\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    \n",
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT I SCENE I.\n",
      "Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others So shaken as we are, so wan with care, Find we a time for frighted peace to pant, And breathe short-winded accents of new broils To be commenced in strands afar remote.\n",
      "No more the thirsty entrance of this soil Shall daub her lips with her own children's blood, Nor more shall trenching war channel her fields, Nor bruise her flowerets with the armed hoofs Of hostile paces: those opposed eyes, Which, like the meteors of a troubled heaven, All of one nature, of one substance bred, Did lately meet in the intestine shock And furious close of civil butchery Shall now, in mutual well-beseeming ranks, March all one way and be no more opposed Against acquaintance, kindred and allies: The edge of war, like an ill-sheathed knife, No more shall cut his master.\n",
      "As far as to the sepulchre of Christ, Whose soldier now, under whose blessed cross We are impressed and engaged to fight, Forthwith a power of English shall we levy, Whose arms were moulded in their mothers' womb To chase these pagans in those holy fields Over whose acres walk'd those blessed feet Which fourteen hundred years ago were nail'd For our advantage on the bitter cross.\n",
      "But this our purpose now is twelve month old, And bootless 'tis to tell you we will go: Therefore we meet not now.\n",
      "Of you, my gentle cousin Westmoreland, What yesternight our council did decree In forwarding this dear expedience.\n",
      "My liege, this haste was hot in question, And many limits of the charge set down But yesternight: when all athwart there came A post from Wales loaden with heavy news, Whose worst was, that the noble Mortimer, Leading the men of Herefordshire to fight Against the irregular and wild Glendower, Was by the rude hands of that Welshman taken, A thousand of his people butchered, Upon whose dead corpse there was such misuse, Such beastly shameless transformation, By those Welshwomen done as may not be Without much shame retold or spoken of.\n",
      "It seems then that the tidings of this broil Brake off our business for the Holy Land.\n",
      "This match'd with other did, my gracious lord, For more uneven and unwelcome news Came from the north and thus it did import: On Holy-rood day, the gallant Hotspur there, Young Harry Percy and brave Archibald, That ever-valiant and approved Scot, At Holmedon met, Where they did spend a sad and bloody hour, As by discharge of their artillery, And shape of likelihood, the news was told, For he that brought them, in the very heat And pride of their contention did take horse, Uncertain of the issue any way.\n",
      "Here is a dear, a true industrious friend, Sir Walter Blunt, new lighted from his horse.\n"
     ]
    }
   ],
   "source": [
    "# The shakespeare.txt contains multiple lines of text, quoted with double quotes.\n",
    "# Let's read each line and remove the double quotes, then append the line to a list.\n",
    "read_lines = []\n",
    "with open('Data/shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # Regular expression to remove double quotes at beginning and end of line\n",
    "        line = re.sub(r'^\"|\"\\n$', '', line)\n",
    "        read_lines.append(line)\n",
    "        \n",
    "# We will combine lines that end with a dot as a single sentence.\n",
    "sentences = []\n",
    "current_sentence = ''\n",
    "for i in range(len(read_lines)):\n",
    "    # Find the first index of a dot in the line. If it exists, append the line to the current sentence containing the dot, cut off the rest of the line, decrease the i counter by 1 and continue.\n",
    "    # If it doesn't exist, append the line to the current sentence and continue.\n",
    "    dot_index = read_lines[i].find('.')\n",
    "    \n",
    "    if dot_index != -1:\n",
    "        # Append the line to the current sentence\n",
    "        current_sentence +=  ' ' + read_lines[i][:dot_index+1]\n",
    "        # Cut off the rest of the line\n",
    "        read_lines[i] = read_lines[i][dot_index+1:]\n",
    "        # Append the current sentence to the list of sentences\n",
    "        sentences.append(current_sentence.strip())\n",
    "        # Reset the current sentence\n",
    "        current_sentence = ''\n",
    "        # Decrease the i counter by 1 so that the current line is processed again\n",
    "        i -= 1\n",
    "    else:\n",
    "        # Append the line to the current sentence\n",
    "        current_sentence += ' ' + read_lines[i]\n",
    "        \n",
    "# If the current sentence is not empty, append it to the list of sentences as well\n",
    "if current_sentence != '':\n",
    "    sentences.append(current_sentence.strip())\n",
    "    \n",
    "# Let's print the first 10 sentences\n",
    "for i in range(10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy tokenizer to clean the text\n",
    "def spacy_text_clean(text):\n",
    "    \"\"\"\n",
    "    This function uses the spacy tokenizer to clean the text\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        tokens: A list of tokens that have been cleaned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a spacy object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.lower_)\n",
    "        elif token.is_punct:\n",
    "            tokens.append(token.text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'world', '!', 'this', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "text = \"Hello, world! This is a test.\"\n",
    "print(spacy_text_clean(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 341/341 [00:55<00:00,  6.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the token list\n",
    "cleaned_tokens = []\n",
    "\n",
    "# For each batch of sentences, append them then clean them\n",
    "batch = 100\n",
    "for i in tqdm(range(0, len(sentences), batch)):\n",
    "    cleaned_tokens.extend(spacy_text_clean(' '.join(sentences[i:i+batch])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "946795\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the cleaned tokens\n",
    "print(len(cleaned_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['act', 'i', 'scene', 'enter', 'king', 'henry', ',', 'lord', 'john', 'of', 'lancaster', ',', 'the', 'earl', 'of', 'westmoreland', ',', 'sir', 'walter', 'blunt', ',', 'and', 'others', 'so', 'shaken', 'as', 'we', 'are', ',', 'so', 'wan', 'with', 'care', ',', 'find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant', ',', 'and', 'breathe', 'short', '-', 'winded', 'accents', 'of', 'new', 'broils', 'to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote', '.', 'no', 'more', 'the', 'thirsty', 'entrance', 'of', 'this', 'soil', 'shall', 'daub', 'her', 'lips', 'with', 'her', 'own', 'children', 'blood', ',', 'nor', 'more', 'shall', 'trenching', 'war', 'channel', 'her', 'fields', ',', 'nor', 'bruise', 'her', 'flowerets', 'with', 'the', 'armed', 'hoofs', 'of', 'hostile', 'paces', ':', 'those', 'opposed', 'eyes', ',', 'which', ',', 'like', 'the', 'meteors', 'of', 'a', 'troubled', 'heaven', ',', 'all', 'of', 'one', 'nature', ',', 'of', 'one', 'substance', 'bred', ',', 'did', 'lately', 'meet', 'in', 'the', 'intestine', 'shock', 'and', 'furious', 'close', 'of', 'civil', 'butchery', 'shall', 'now', ',', 'in', 'mutual', 'well', '-', 'beseeming', 'ranks', ',', 'march', 'all', 'one', 'way', 'and', 'be', 'no', 'more', 'opposed', 'against', 'acquaintance', ',', 'kindred', 'and', 'allies', ':', 'the', 'edge', 'of', 'war', ',', 'like', 'an', 'ill', '-', 'sheathed', 'knife', ',', 'no', 'more', 'shall', 'cut', 'his', 'master', '.', 'as', 'far', 'as', 'to', 'the', 'sepulchre', 'of', 'christ', ',', 'whose', 'soldier', 'now', ',', 'under', 'whose', 'blessed', 'cross', 'we', 'are', 'impressed', 'and', 'engaged', 'to', 'fight', ',', 'forthwith', 'a', 'power', 'of', 'english', 'shall', 'we', 'levy', ',', 'whose', 'arms', 'were', 'moulded', 'in', 'their', 'mothers', \"'\", 'womb', 'to', 'chase', 'these', 'pagans', 'in', 'those', 'holy', 'fields', 'over', 'whose', 'acres', 'those', 'blessed', 'feet', 'which', 'fourteen', 'hundred', 'years', 'ago', 'were', 'for', 'our', 'advantage', 'on', 'the', 'bitter', 'cross', '.', 'but', 'this', 'our', 'purpose', 'now', 'is', 'twelve', 'month', 'old', ',', 'and', 'bootless', \"'\", 'tis', 'to', 'tell', 'you', 'we', 'will', 'go', ':', 'therefore', 'we', 'meet', 'not', 'now', '.', 'of', 'you', ',', 'my', 'gentle', 'cousin', 'westmoreland', ',', 'what', 'yesternight', 'our', 'council', 'did', 'decree', 'in', 'forwarding', 'this', 'dear', 'expedience', '.', 'my', 'liege', ',', 'this', 'haste', 'was', 'hot', 'in', 'question', ',', 'and', 'many', 'limits', 'of', 'the', 'charge', 'set', 'down', 'but', 'yesternight', ':', 'when', 'all', 'athwart', 'there', 'came', 'a', 'post', 'from', 'wales', 'loaden', 'with', 'heavy', 'news', ',', 'whose', 'worst', 'was', ',', 'that', 'the', 'noble', 'mortimer', ',', 'leading', 'the', 'men', 'of', 'herefordshire', 'to', 'fight', 'against', 'the', 'irregular', 'and', 'wild', 'glendower', ',', 'was', 'by', 'the', 'rude', 'hands', 'of', 'that', 'welshman', 'taken', ',', 'a', 'thousand', 'of', 'his', 'people', 'butchered', ',', 'upon', 'whose', 'dead', 'corpse', 'there', 'was', 'such', 'misuse', ',', 'such', 'beastly', 'shameless', 'transformation', ',', 'by', 'those', 'welshwomen', 'done', 'as', 'may', 'not', 'be', 'without', 'much', 'shame', 'retold', 'or', 'spoken', 'of', '.', 'it', 'seems', 'then', 'that', 'the', 'tidings', 'of', 'this', 'broil', 'brake', 'off', 'our', 'business', 'for', 'the', 'holy', 'land', '.', 'this', 'with', 'other', 'did', ',', 'my', 'gracious', 'lord', ',', 'for', 'more', 'uneven', 'and', 'unwelcome', 'news', 'came', 'from', 'the', 'north', 'and', 'thus', 'it', 'did', 'import', ':', 'on', 'holy', '-', 'rood', 'day', ',', 'the', 'gallant', 'hotspur', 'there', ',', 'young', 'harry', 'percy', 'and', 'brave', 'archibald', ',', 'that', 'ever', '-', 'valiant', 'and', 'approved', 'scot', ',', 'at', 'holmedon', 'met', ',', 'where', 'they', 'did', 'spend', 'a', 'sad', 'and', 'bloody', 'hour', ',', 'as', 'by', 'discharge', 'of', 'their', 'artillery', ',', 'and', 'shape', 'of', 'likelihood', ',', 'the', 'news', 'was', 'told', ',', 'for', 'he', 'that', 'brought', 'them', ',', 'in', 'the', 'very', 'heat', 'and', 'pride', 'of', 'their', 'contention', 'did', 'take', 'horse', ',', 'uncertain', 'of', 'the', 'issue', 'any', 'way', '.', 'here', 'is', 'a', 'dear', ',', 'a', 'true', 'industrious', 'friend', ',', 'sir', 'walter', 'blunt', ',', 'new', 'lighted', 'from', 'his', 'horse', '.', 'with', 'the', 'variation', 'of', 'each', 'soil', 'betwixt', 'that', 'holmedon', 'and', 'this', 'seat', 'of', 'ours', ',', 'and', 'he', 'hath', 'brought', 'us', 'smooth', 'and', 'welcome', 'news', '.', 'the', 'earl', 'of', 'douglas', 'is', 'discomfited', ':', 'ten', 'thousand', 'bold', 'scots', ',', 'two', 'and', 'twenty', 'knights', ',', 'in', 'their', 'own', 'blood', 'did', 'sir', 'walter', 'see', 'on', 'holmedon', 'plains', '.', 'mordake', 'the', 'earl', 'of', 'fife', ',', 'and', 'eldest', 'son', 'to', 'beaten', 'douglas', ',', 'and', 'the', 'earl', 'of', 'athol', ',', 'of', 'murray', ',', 'angus', ',', 'and', 'menteith', ':', 'and', 'is', 'not', 'this', 'an', 'honourable', 'spoil', '?', 'a', 'gallant', 'prize', '?', 'ha', ',', 'cousin', ',', 'is', 'it', 'not', '?', 'in', 'faith', ',', 'it', 'is', 'a', 'conquest', 'for', 'a', 'prince', 'to', 'boast', 'of', '.', 'yea', ',', 'there', 'thou', 'makest', 'me', 'sad', 'and', 'makest', 'me', 'sin', 'in', 'envy', 'that', 'my', 'lord', 'northumberland', 'should', 'be', 'the', 'father', 'to', 'so', 'blest', 'a', 'son', ',', 'a', 'son', 'who', 'is', 'the', 'theme', 'of', 'honour', 'tongue', ',', 'amongst', 'a', 'grove', ',', 'the', 'very', 'straightest', 'plant', ',', 'who', 'is', 'sweet', 'fortune', 'minion', 'and', 'her', 'pride', ':', 'whilst', 'i', ',', 'by', 'looking', 'on', 'the', 'praise', 'of', 'him', ',', 'see', 'riot', 'and', 'dishonour', 'stain', 'the', 'brow', 'of', 'my', 'young', 'harry', '.', 'that', 'some', 'night', '-', 'tripping', 'fairy', 'had', 'exchanged', 'in', 'cradle', '-', 'clothes', 'our', 'children', 'where', 'they', 'lay', ',', 'and', 'mine', 'percy', ',', 'his', 'plantagenet', '!', 'then', 'would', 'i', 'have', 'his', 'harry', ',', 'and', 'he', 'mine', '.', 'but', 'let', 'him', 'from', 'my', 'thoughts', '.', 'of', 'this', 'young', 'percy', 'pride', '?', 'the', 'prisoners', ',', 'which', 'he', 'in', 'this', 'adventure', 'hath', 'surprised', ',', 'to', 'his', 'own', 'use', 'he', 'keeps', ',', 'and', 'sends', 'me', 'word', ',', 'i', 'shall', 'have', 'none', 'but', 'mordake', 'earl', 'of', 'fife', '.', 'this', 'is', 'his', 'uncle', 'teaching', ',', 'this', 'is', 'worcester', ',', 'malevolent', 'to', 'you', 'in', 'all', 'aspects', ',', 'which', 'makes', 'him', 'prune', 'himself', ',', 'and', 'bristle', 'up', 'the', 'crest', 'of', 'youth', 'against', 'your', 'dignity', '.', 'but', 'i', 'have', 'sent', 'for', 'him', 'to', 'answer', 'this', ',', 'and', 'for', 'this', 'cause', 'awhile', 'we', 'must', 'neglect', 'our', 'holy', 'purpose', 'to', 'jerusalem', '.', 'cousin', ',', 'on', 'wednesday', 'next', 'our', 'council', 'we', 'will', 'hold', 'at', 'windsor', ',', 'so', 'inform', 'the', 'lords', ':', 'but', 'come', 'yourself', 'with', 'speed', 'to', 'us', 'again', ',', 'for', 'more', 'is', 'to', 'be', 'said', 'and', 'to', 'be', 'done', 'than', 'out', 'of', 'anger', 'can', 'be', 'uttered', '.', 'i', 'will', ',', 'my', 'liege', '.', 'exeunt', 'scene', 'ii', '.', 'enter', 'the', 'prince', 'of', 'wales', 'and', 'falstaff', 'now', ',', 'hal', ',', 'what', 'time', 'of', 'day', 'is', 'it', ',', 'lad', '?', 'thou', 'art', 'so', 'fat', '-', 'witted', ',', 'with', 'drinking', 'of', 'old', 'sack', 'and', 'unbuttoning', 'thee', 'after', 'supper', 'and', 'sleeping', 'upon', 'benches', 'after', 'noon', ',', 'that', 'thou', 'hast', 'forgotten', 'to', 'demand', 'that', 'truly', 'which', 'thou', 'wouldst', 'truly', 'know', '.', 'what', 'a', 'devil']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 1000 tokens\n",
    "print(cleaned_tokens[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary of words from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a vocabulary from the tokens, with cutoff of 25\n",
    "vocab = vocab_utils.vocabulary_from_tokens(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2517\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the vocabulary\n",
    "print(\"Vocabulary size:\", vocab['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 words in the vocabulary: ['<UNK>', '<PAD>', '<EOS>', 'act', 'i', 'scene', 'enter', 'king', 'henry', ',', 'lord', 'john', 'of', 'lancaster', 'the', 'earl', 'westmoreland', 'sir', 'blunt', 'and', 'others', 'so', 'as', 'we', 'are', 'with', 'care', 'find', 'a', 'time', 'for', 'peace', 'to', 'breathe', 'short', '-', 'new', 'be', 'in', '.', 'no', 'more', 'this', 'shall', 'her', 'lips', 'own', 'children', 'blood', 'nor', 'war', 'fields', 'armed', ':', 'those', 'opposed', 'eyes', 'which', 'like', 'troubled', 'heaven', 'all', 'one', 'nature', 'substance', 'bred', 'did', 'lately', 'meet', 'close', 'civil', 'now', 'well', 'march', 'way', 'against', 'acquaintance', 'kindred', 'edge', 'an', 'ill', 'knife', 'cut', 'his', 'master', 'far', 'whose', 'soldier', 'under', 'blessed', 'cross', 'fight', 'power', 'english', 'arms', 'were', 'their', \"'\", 'womb', 'chase']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 100 words in the vocabulary\n",
    "print(\"First 100 words in the vocabulary:\", list(vocab['word2idx'].keys())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context_target(sentence, window_size=2):\n",
    "    \"\"\"\n",
    "    This function extracts the context and target words from a sentence\n",
    "    \n",
    "    Args:\n",
    "        sentence (list): A list of tokens in the sentence\n",
    "        window_size (int): The size of the context window\n",
    "        \n",
    "    Returns:\n",
    "        context_target_pairs (list): A list of tuples containing the context and target words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of context and target pairs\n",
    "    context_target_pairs = []\n",
    "    \n",
    "    # For each word in the sentence\n",
    "    for i in range(len(sentence)):\n",
    "        # Extract the context words\n",
    "        context_words = []\n",
    "        for j in range(i-window_size, i+window_size+1):\n",
    "            if j != i and j >= 0 and j < len(sentence):\n",
    "                context_words.append(sentence[j])\n",
    "        \n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "        \n",
    "        # Append the context and target words to the list of pairs only if the context words containing enough words\n",
    "        if len(context_words) == 2*window_size:\n",
    "            context_target_pairs.append((context_words, target_word))\n",
    "        \n",
    "    return context_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others So shaken as we are, so wan with care, Find we a time for frighted peace to pant, And breathe short-winded accents of new broils To be commenced in strands afar remote.\n",
      "['enter', 'king', 'henry', ',', 'lord', 'john', 'of', 'lancaster', ',', 'the', 'earl', 'of', 'westmoreland', ',', 'sir', 'walter', 'blunt', ',', 'and', 'others', 'so', 'shaken', 'as', 'we', 'are', ',', 'so', 'wan', 'with', 'care', ',', 'find', 'we', 'a', 'time', 'for', 'frighted', 'peace', 'to', 'pant', ',', 'and', 'breathe', 'short', '-', 'winded', 'accents', 'of', 'new', 'broils', 'to', 'be', 'commenced', 'in', 'strands', 'afar', 'remote', '.']\n",
      "[(['enter', 'king', ',', 'lord'], 'henry'), (['king', 'henry', 'lord', 'john'], ','), (['henry', ',', 'john', 'of'], 'lord'), ([',', 'lord', 'of', 'lancaster'], 'john'), (['lord', 'john', 'lancaster', ','], 'of'), (['john', 'of', ',', 'the'], 'lancaster'), (['of', 'lancaster', 'the', 'earl'], ','), (['lancaster', ',', 'earl', 'of'], 'the'), ([',', 'the', 'of', 'westmoreland'], 'earl'), (['the', 'earl', 'westmoreland', ','], 'of'), (['earl', 'of', ',', 'sir'], 'westmoreland'), (['of', 'westmoreland', 'sir', 'walter'], ','), (['westmoreland', ',', 'walter', 'blunt'], 'sir'), ([',', 'sir', 'blunt', ','], 'walter'), (['sir', 'walter', ',', 'and'], 'blunt'), (['walter', 'blunt', 'and', 'others'], ','), (['blunt', ',', 'others', 'so'], 'and'), ([',', 'and', 'so', 'shaken'], 'others'), (['and', 'others', 'shaken', 'as'], 'so'), (['others', 'so', 'as', 'we'], 'shaken'), (['so', 'shaken', 'we', 'are'], 'as'), (['shaken', 'as', 'are', ','], 'we'), (['as', 'we', ',', 'so'], 'are'), (['we', 'are', 'so', 'wan'], ','), (['are', ',', 'wan', 'with'], 'so'), ([',', 'so', 'with', 'care'], 'wan'), (['so', 'wan', 'care', ','], 'with'), (['wan', 'with', ',', 'find'], 'care'), (['with', 'care', 'find', 'we'], ','), (['care', ',', 'we', 'a'], 'find'), ([',', 'find', 'a', 'time'], 'we'), (['find', 'we', 'time', 'for'], 'a'), (['we', 'a', 'for', 'frighted'], 'time'), (['a', 'time', 'frighted', 'peace'], 'for'), (['time', 'for', 'peace', 'to'], 'frighted'), (['for', 'frighted', 'to', 'pant'], 'peace'), (['frighted', 'peace', 'pant', ','], 'to'), (['peace', 'to', ',', 'and'], 'pant'), (['to', 'pant', 'and', 'breathe'], ','), (['pant', ',', 'breathe', 'short'], 'and'), ([',', 'and', 'short', '-'], 'breathe'), (['and', 'breathe', '-', 'winded'], 'short'), (['breathe', 'short', 'winded', 'accents'], '-'), (['short', '-', 'accents', 'of'], 'winded'), (['-', 'winded', 'of', 'new'], 'accents'), (['winded', 'accents', 'new', 'broils'], 'of'), (['accents', 'of', 'broils', 'to'], 'new'), (['of', 'new', 'to', 'be'], 'broils'), (['new', 'broils', 'be', 'commenced'], 'to'), (['broils', 'to', 'commenced', 'in'], 'be'), (['to', 'be', 'in', 'strands'], 'commenced'), (['be', 'commenced', 'strands', 'afar'], 'in'), (['commenced', 'in', 'afar', 'remote'], 'strands'), (['in', 'strands', 'remote', '.'], 'afar')]\n"
     ]
    }
   ],
   "source": [
    "## Test the function\n",
    "sentence = sentences[1]\n",
    "print(sentence)\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = spacy_text_clean(sentence)\n",
    "print(tokens)\n",
    "\n",
    "# Extract the context and target words\n",
    "context_target_pairs = extract_context_target(tokens)\n",
    "print(context_target_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function create a one-hot vector from a word\n",
    "def one_hot_vector(vocab, word):\n",
    "    \"\"\"\n",
    "    This function creates a one-hot vector from a word\n",
    "    \n",
    "    Args:\n",
    "        vocab (dict): The vocabulary dictionary\n",
    "        word (str): The word to be converted to a one-hot vector\n",
    "        \n",
    "    Returns:\n",
    "        one_hot (np.array): A one-hot vector representation of the word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the one-hot vector\n",
    "    one_hot = np.zeros(vocab['count'])\n",
    "    \n",
    "    # Set the index of the word to 1\n",
    "    one_hot[vocab_utils.lookup_token(vocab, word)] = 1\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "word = 'king'\n",
    "one_hot = one_hot_vector(vocab, word)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_target_pairs(sentences, window_size=2):\n",
    "    \"\"\"\n",
    "    This function generates the context and target pairs from a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        sentences (list): A list of sentences\n",
    "        window_size (int): The size of the context window\n",
    "        \n",
    "    Returns:\n",
    "        context_target_pairs (list): A list of tuples containing the context and target words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of context and target pairs\n",
    "    context_target_pairs = []\n",
    "    \n",
    "    # For each sentence\n",
    "    print(f'Generating context and target pairs from {len(sentences)} sentences')\n",
    "    for sentence in tqdm(sentences):\n",
    "        # Tokenize the sentence\n",
    "        tokens = spacy_text_clean(sentence)\n",
    "        \n",
    "        # Extract the context and target words\n",
    "        pairs = extract_context_target(tokens, window_size)\n",
    "        \n",
    "        # Append the pairs to the list of context and target pairs\n",
    "        context_target_pairs.extend(pairs)\n",
    "        \n",
    "    # Shuffle the pairs\n",
    "    np.random.shuffle(context_target_pairs)\n",
    "        \n",
    "    return context_target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_target_to_one_hot(vocab, context_target_pairs, batch_size=128):\n",
    "    \"\"\"\n",
    "    This function yields a batch of context and target one-hot vectors from a list of sentences\n",
    "    \n",
    "    Args:\n",
    "        vocab (dict): The vocabulary dictionary\n",
    "        context_target_pairs (list): A list of tuples containing the context and target words\n",
    "        batch_size (int): The batch size\n",
    "        \n",
    "    Returns:\n",
    "        yield (tuple): A tuple containing the context and target one-hot vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    # For each batch of context and target pairs\n",
    "    for i in range(0, len(context_target_pairs), batch_size):\n",
    "        # If i is equal to or greater than the length of the context and target pairs, break\n",
    "        if i >= len(context_target_pairs):\n",
    "            break\n",
    "        \n",
    "        # Initialize the context and target one-hot vectors\n",
    "        context_one_hot = np.zeros((batch_size, vocab['count']))\n",
    "        target_one_hot = np.zeros((batch_size, vocab['count']))\n",
    "        \n",
    "        # For each pair in the batch\n",
    "        for j in range(i, i+batch_size):\n",
    "            # Avoid index out of bounds\n",
    "            if j >= len(context_target_pairs):\n",
    "                # Change j to the last index\n",
    "                j = len(context_target_pairs)-1\n",
    "            \n",
    "            # Extract the context and target words\n",
    "            context_words, target_word = context_target_pairs[j]\n",
    "            \n",
    "            # Create the context one-hot vector\n",
    "            context_one_hot[j-i] = np.mean([one_hot_vector(vocab, word) for word in context_words], axis=0)\n",
    "            \n",
    "            # Create the target one-hot vector\n",
    "            target_one_hot[j-i] = one_hot_vector(vocab, target_word)\n",
    "        \n",
    "        # Preferably, yield the context and target one-hot vectors as torch tensors on the GPU\n",
    "        # yield (torch.from_numpy(context_one_hot).float(), torch.from_numpy(target_one_hot).float())\n",
    "        if torch.cuda.is_available():\n",
    "            yield (torch.from_numpy(context_one_hot).float().cuda(), torch.from_numpy(target_one_hot).float().cuda())\n",
    "        else:\n",
    "            yield (torch.from_numpy(context_one_hot).float(), torch.from_numpy(target_one_hot).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CBOW model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(vocab_size, embedding_dim):\n",
    "    \"\"\"\n",
    "    This function initializes a continuous bag of words model\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary\n",
    "        embedding_dim (int): The size of the embedding dimension\n",
    "        \n",
    "    Returns:\n",
    "        model: The initialized pytorch model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the model\n",
    "    model = nn.Sequential(\n",
    "        # First layer has vocab_size inputs and embedding_dim outputs\n",
    "        nn.Linear(vocab_size, embedding_dim),\n",
    "        nn.ReLU(),\n",
    "        # Second layer has embedding_dim inputs and vocab_size outputs\n",
    "        nn.Linear(embedding_dim, vocab_size)\n",
    "    )\n",
    "    \n",
    "    # Preferably, move the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = initialize_model(vocab['count'], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2517, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=2517, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_context_target_pairs, validate_context_target_pairs, epochs=10, lr=0.001, batch_size=100):\n",
    "    \"\"\"\n",
    "    This function trains a pytorch model on a dataset\n",
    "    \n",
    "    Args:\n",
    "        model: The pytorch model to be trained\n",
    "        train_context_target_pairs (list): A list of tuples containing the context and target words for training\n",
    "        validate_context_target_pairs (list): A list of tuples containing the context and target words for validation\n",
    "        epochs (int): The number of epochs\n",
    "        lr (float): The learning rate\n",
    "        batch_size (int): The batch size\n",
    "        \n",
    "    Returns:\n",
    "        model: The trained pytorch model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store the training and validation losses\n",
    "    train_losses = []\n",
    "    validate_losses = []\n",
    "\n",
    "    # Define the loss function\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # For each epoch\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize the training loss\n",
    "        train_loss = 0\n",
    "        \n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # For each batch of context and target one-hot vectors\n",
    "        for context, target in context_target_to_one_hot(vocab, train_context_target_pairs, batch_size=batch_size):\n",
    "            # Reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model(context)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(predictions, target)\n",
    "            \n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the loss to the training loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Normalize the training loss\n",
    "        train_loss /= len(train_context_target_pairs)\n",
    "        \n",
    "        # Add the training loss to the list of training losses\n",
    "        train_losses.append(train_loss)\n",
    "            \n",
    "        # Print the training loss\n",
    "        print(\"Epoch:\", epoch+1, \"Training loss:\", train_loss)\n",
    "        \n",
    "        # Initialize the validation loss\n",
    "        validate_loss = 0\n",
    "        \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # For each batch of context and target one-hot vectors\n",
    "        for context, target in context_target_to_one_hot(vocab, validate_context_target_pairs, batch_size=batch_size):\n",
    "            # Make predictions\n",
    "            predictions = model(context)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = loss_function(predictions, target)\n",
    "            \n",
    "            # Add the loss to the validation loss\n",
    "            validate_loss += loss.item()\n",
    "            \n",
    "        # Normalize the validation loss\n",
    "        validate_loss /= len(validate_context_target_pairs)\n",
    "        \n",
    "        # Add the validation loss to the list of validation losses\n",
    "        validate_losses.append(validate_loss)\n",
    "            \n",
    "        # Print the validation loss\n",
    "        print(\"Epoch:\", epoch+1, \"Validation loss:\", validate_loss)\n",
    "        \n",
    "    return model, train_losses, validate_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences into training and validation sets\n",
    "shuffle_indices = np.random.permutation(len(sentences))\n",
    "train_sentences = [sentences[i] for i in shuffle_indices[:int(len(sentences)*0.8)]]\n",
    "validate_sentences = [sentences[i] for i in shuffle_indices[int(len(sentences)*0.8):]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating context and target pairs from 27232 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27232/27232 [01:36<00:00, 282.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating context and target pairs from 6808 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6808/6808 [00:25<00:00, 267.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate the context and target pairs for the training and validation sets\n",
    "train_context_target_pairs = generate_context_target_pairs(train_sentences)\n",
    "validate_context_target_pairs = generate_context_target_pairs(validate_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training loss: 8.837177999125867e-05\n",
      "Epoch: 1 Validation loss: 1.286609402516686e-05\n",
      "Epoch: 2 Training loss: 1.2809699251197101e-05\n",
      "Epoch: 2 Validation loss: 1.2712779676341018e-05\n",
      "Epoch: 3 Training loss: 1.2532187426502766e-05\n",
      "Epoch: 3 Validation loss: 1.2238829258572894e-05\n",
      "Epoch: 4 Training loss: 1.1957012434312268e-05\n",
      "Epoch: 4 Validation loss: 1.1693672537015175e-05\n",
      "Epoch: 5 Training loss: 1.1503789787238064e-05\n",
      "Epoch: 5 Validation loss: 1.1378469986710027e-05\n",
      "Epoch: 6 Training loss: 1.1236062804687195e-05\n",
      "Epoch: 6 Validation loss: 1.1207570756432245e-05\n",
      "Epoch: 7 Training loss: 1.1063133389462257e-05\n",
      "Epoch: 7 Validation loss: 1.1098585523647353e-05\n",
      "Epoch: 8 Training loss: 1.0933104906055643e-05\n",
      "Epoch: 8 Validation loss: 1.1019999692733033e-05\n",
      "Epoch: 9 Training loss: 1.082608949329442e-05\n",
      "Epoch: 9 Validation loss: 1.0959479388047216e-05\n",
      "Epoch: 10 Training loss: 1.0734578504406137e-05\n",
      "Epoch: 10 Validation loss: 1.091228175370547e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, train_losses, validate_losses = train_model(model, train_context_target_pairs, validate_context_target_pairs, epochs=10, lr=0.001, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw the training and validation losses\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(validate_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Word Embeddings by calculating mean of the W1 and W2 weight matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model):\n",
    "    \"\"\"\n",
    "    This function gets the embedding matrix of all words, by calculating the mean of the weight matrix of the first layer of the model and the second layer of the model\n",
    "    \n",
    "    Args:\n",
    "        model: The pytorch model\n",
    "        \n",
    "    Returns:\n",
    "        embedding (np.array): The embedding of the word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the weight matrix of the first layer\n",
    "    weight1 = model[0].weight.data.cpu().numpy()\n",
    "    \n",
    "    # Get the weight matrix of the second layer\n",
    "    weight2 = model[2].weight.data.cpu().numpy()\n",
    "    \n",
    "    # Calculate the mean of the two weight matrices (transpose first layer weight matrix)\n",
    "    all_embeddings = (weight1.T + weight2) / 2\n",
    "    \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the embedding matrix of all words\n",
    "all_embeddings = get_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2517, 100)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the embedding\n",
    "print(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the embedding of a word\n",
    "def get_word_embedding(word, all_embeddings, vocab):\n",
    "    \"\"\"\n",
    "    This function gets the embedding of a word\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to be embedded\n",
    "        all_embeddings (np.array): The embedding matrix\n",
    "        vocab (dict): The vocabulary dictionary\n",
    "        \n",
    "    Returns:\n",
    "        embedding (np.array): The embedding of the word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the index of the word\n",
    "    index = vocab_utils.lookup_token(vocab, word)\n",
    "    \n",
    "    # Return the embedding\n",
    "    return all_embeddings[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.48076576e-02  1.64634913e-01 -6.74484015e-01  1.80700362e-01\n",
      "  3.82556021e-01  1.94576442e-01 -8.54499459e-01 -1.92681588e-02\n",
      " -1.64464399e-01 -1.99554235e-01 -9.03800488e-01  2.21777335e-01\n",
      " -6.02526665e-02  5.71528733e-01 -1.07038945e-01  1.97753385e-02\n",
      " -3.15813780e-01 -3.96371603e-01 -2.82632619e-01 -5.75842559e-02\n",
      " -7.52269709e-03 -3.42431664e-01  5.19586980e-01  7.50028908e-01\n",
      " -1.09954268e-01 -5.30472577e-01 -1.09134391e-01  8.05588905e-04\n",
      "  3.11922461e-01  5.23508430e-01 -3.77018511e-01 -2.51066595e-01\n",
      " -3.50451469e-01  1.51424468e-01  1.40312240e-01  5.22333011e-02\n",
      "  2.24427551e-01 -1.50186300e-01 -6.31714940e-01 -4.16963637e-01\n",
      "  7.55169332e-01  2.98405230e-01 -3.85400355e-02  1.47878844e-02\n",
      " -6.77050114e-01  5.15243039e-04 -2.90218115e-01 -4.15866464e-01\n",
      "  2.00566918e-01  3.81082296e-04  1.43253684e-01 -2.24573761e-01\n",
      "  3.27882975e-01  1.49971470e-01  7.43446872e-05 -3.96475017e-01\n",
      "  1.73591018e-01 -3.62475097e-01 -5.25295556e-01  3.10391665e-01\n",
      " -2.18497198e-02 -7.98330293e-04 -9.35203582e-03  8.13850015e-03\n",
      "  5.79257369e-01 -2.59860903e-02 -3.87468308e-01 -2.95446068e-02\n",
      " -1.29440188e-01 -6.60244823e-02 -3.94941449e-01 -2.91309088e-01\n",
      " -1.10780299e-01 -6.41239285e-02  2.82086413e-02 -5.09153485e-01\n",
      "  1.84433311e-02 -1.51971027e-01  2.85992846e-02 -1.80107690e-02\n",
      " -1.96290016e-03  5.09982228e-01  1.14846778e+00 -8.90296698e-02\n",
      "  4.31429803e-01 -6.11693084e-01  2.53771216e-01 -1.14639655e-01\n",
      " -1.76414568e-02 -2.11395457e-01 -1.53384358e-01 -1.38779044e-01\n",
      "  2.05148414e-01  4.15542722e-03 -1.33194074e-01 -1.25623211e-01\n",
      " -6.48836792e-02 -4.22856063e-01  5.17688096e-01  2.95736313e-01]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "word = 'king'\n",
    "word_embedding = get_word_embedding(word, all_embeddings, vocab)\n",
    "print(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the most similar words to a word\n",
    "def get_most_similar_words(word, all_embeddings, vocab, n=10):\n",
    "    \"\"\"\n",
    "    This function gets the most similar words to a word\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to be embedded\n",
    "        all_embeddings (np.array): The embedding matrix\n",
    "        vocab (dict): The vocabulary dictionary\n",
    "        n (int): The number of similar words to return\n",
    "        \n",
    "    Returns:\n",
    "        similar_words (list): A list of tuples containing the similar words and their cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the embedding of the word\n",
    "    word_embedding = get_word_embedding(word, all_embeddings, vocab)\n",
    "    \n",
    "    # Calculate the cosine similarity between the word and all other words\n",
    "    cosine_similarity = np.dot(all_embeddings, word_embedding) / (np.linalg.norm(all_embeddings, axis=1) * np.linalg.norm(word_embedding))\n",
    "    \n",
    "    # Get the indices of the most similar words\n",
    "    similar_word_indices = np.argsort(cosine_similarity)[::-1][1:n+1]\n",
    "    \n",
    "    # Get the similar words and their cosine similarity\n",
    "    similar_words = [(vocab['idx2word'][index], cosine_similarity[index]) for index in similar_word_indices]\n",
    "    \n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('henry', 0.8662488), ('richard', 0.84505343), ('edward', 0.83305484), ('vi', 0.7767554), ('prince', 0.720985), ('england', 0.7085519), ('duke', 0.6807811), ('iii', 0.65871537), ('queen', 0.6483214), ('lear', 0.64747155)]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "word = 'king'\n",
    "similar_words = get_most_similar_words(word, all_embeddings, vocab)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prince', 0.8114015), ('doctor', 0.7786069), ('harry', 0.7740024), ('vi', 0.75712955), ('princess', 0.74393153), ('buckingham', 0.7433236), ('lancaster', 0.7395614), ('suffolk', 0.7394096), ('humphrey', 0.73338836), ('kent', 0.73284763)]\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "word = 'queen'\n",
    "similar_words = get_most_similar_words(word, all_embeddings, vocab)\n",
    "print(similar_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttth_natural_language_processing_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
