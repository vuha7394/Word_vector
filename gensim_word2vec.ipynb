{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# **Word2Vec** with **Gensim**\n",
    "\n",
    "In this Jupyter notebook, we will demonstrate how to use the **Gensim** library to train a **Word2Vec model**. **Word2Vec** is a popular algorithm in **Natural Language Processing (NLP)** that uses **neural networks** to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. \n",
    "\n",
    "**Gensim** is a robust open-source **vector space modeling** and **topic modeling toolkit** implemented in Python. It allows easy handling of large text collections, efficient algorithms, and readily accessible software resources. \n",
    "\n",
    "In this demo, we will walk through the steps of training a **Word2Vec model** with **Gensim**, including **data preprocessing**, **model training**, **parameter tuning**, and finally, how to use the trained model for various **NLP tasks**. \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c39d9f",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "This notebook is inspired by and partially based on the excellent tutorial found at the following link: [Gensim Word2Vec Tutorial](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial). \n",
    "\n",
    "We have adapted and expanded upon the original tutorial to fit the specific needs and context of this notebook. We highly recommend checking out the original tutorial for a more in-depth look at using Gensim for Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77ec0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from time import time\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b3eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the spacy english model is installed\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    \n",
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"]) # disable named entity recognition for speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0593d7bb",
   "metadata": {},
   "source": [
    "### Read and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74a4900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('./Data/simpsons_dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67692152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58ae22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15502244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05bfec57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    0\n",
       "spoken_words          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are any missing values left\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3e64cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131853, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape again\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd1925e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy tokenizer to clean the text\n",
    "def spacy_text_clean(text):\n",
    "    \"\"\"\n",
    "    This function uses the spacy tokenizer to clean the text\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        tokens: A list of tokens that have been cleaned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a spacy object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.lower_)\n",
    "        elif token.is_punct:\n",
    "            tokens.append(token.text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5445fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131853/131853 [05:01<00:00, 437.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the clean token list\n",
    "cleaned_sentences = []\n",
    "\n",
    "for i in tqdm(range(0, len(df))):\n",
    "    cleaned_sentences.append(spacy_text_clean(df['spoken_words'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6e59ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty lists\n",
    "cleaned_sentences = [x for x in cleaned_sentences if x != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b566164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no',\n",
       "  ',',\n",
       "  'actually',\n",
       "  ',',\n",
       "  'it',\n",
       "  'was',\n",
       "  'a',\n",
       "  'little',\n",
       "  'of',\n",
       "  'both',\n",
       "  '.',\n",
       "  'sometimes',\n",
       "  'when',\n",
       "  'a',\n",
       "  'disease',\n",
       "  'is',\n",
       "  'in',\n",
       "  'all',\n",
       "  'the',\n",
       "  'magazines',\n",
       "  'and',\n",
       "  'all',\n",
       "  'the',\n",
       "  'news',\n",
       "  'shows',\n",
       "  ',',\n",
       "  'it',\n",
       "  'only',\n",
       "  'natural',\n",
       "  'that',\n",
       "  'you',\n",
       "  'think',\n",
       "  'you',\n",
       "  'have',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['where', 'bergstrom', '?'],\n",
       " ['i',\n",
       "  'do',\n",
       "  'know',\n",
       "  '.',\n",
       "  'although',\n",
       "  'i',\n",
       "  'sure',\n",
       "  'like',\n",
       "  'to',\n",
       "  'talk',\n",
       "  'to',\n",
       "  'him',\n",
       "  '.',\n",
       "  'he',\n",
       "  'did',\n",
       "  'touch',\n",
       "  'my',\n",
       "  'lesson',\n",
       "  'plan',\n",
       "  '.',\n",
       "  'what',\n",
       "  'did',\n",
       "  'he',\n",
       "  'teach',\n",
       "  'you',\n",
       "  '?'],\n",
       " ['that', 'life', 'is', 'worth', 'living', '.'],\n",
       " ['the',\n",
       "  'polls',\n",
       "  'will',\n",
       "  'be',\n",
       "  'open',\n",
       "  'from',\n",
       "  'now',\n",
       "  'until',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'recess',\n",
       "  '.',\n",
       "  'now',\n",
       "  ',',\n",
       "  'just',\n",
       "  'in',\n",
       "  'case',\n",
       "  'any',\n",
       "  'of',\n",
       "  'you',\n",
       "  'have',\n",
       "  'decided',\n",
       "  'to',\n",
       "  'put',\n",
       "  'any',\n",
       "  'thought',\n",
       "  'into',\n",
       "  'this',\n",
       "  ',',\n",
       "  'we',\n",
       "  'have',\n",
       "  'our',\n",
       "  'final',\n",
       "  'statements',\n",
       "  '.',\n",
       "  'martin',\n",
       "  '?'],\n",
       " ['i', 'do', 'think', 'there', 'anything', 'left', 'to', 'say', '.'],\n",
       " ['bart', '?'],\n",
       " ['victory', 'party', 'under', 'the', 'slide', '!'],\n",
       " ['bergstrom', '!', 'bergstrom', '!'],\n",
       " ['hey',\n",
       "  ',',\n",
       "  'hey',\n",
       "  ',',\n",
       "  'he',\n",
       "  'moved',\n",
       "  'out',\n",
       "  'this',\n",
       "  'morning',\n",
       "  '.',\n",
       "  'he',\n",
       "  'must',\n",
       "  'have',\n",
       "  'a',\n",
       "  'new',\n",
       "  'job',\n",
       "  '--',\n",
       "  'he',\n",
       "  'took',\n",
       "  'his',\n",
       "  'copernicus',\n",
       "  'costume',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 10 sentences\n",
    "cleaned_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbed9f",
   "metadata": {},
   "source": [
    "### Bigrams and Phrases\n",
    "\n",
    "In the context of Natural Language Processing (NLP), a bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. For example, given the sentence \"I love to play football\", the bigrams would be: \"I love\", \"love to\", \"to play\", \"play football\".\n",
    "\n",
    "The `Phrases` model in Gensim is a simple and efficient way to create bigrams. It scans over the provided text data to find common phrases - that is, bigrams that appear more frequently together than you would expect by chance. \n",
    "\n",
    "For example, in a corpus of text about football, the words \"penalty\" and \"kick\" might frequently appear together in that order, and thus would be recognized as a common phrase and represented as a single token, \"penalty_kick\".\n",
    "\n",
    "Using bigrams (or larger n-grams like trigrams, etc.) can help capture important context and improve the performance of many NLP tasks, such as language modeling, machine translation, and information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ddae0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(cleaned_sentences, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4286dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f3ba88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = bigram[cleaned_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0739be1",
   "metadata": {},
   "source": [
    "### Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e83a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a0f8d6",
   "metadata": {},
   "source": [
    "Here's what each parameter in the `Word2Vec` function does:\n",
    "\n",
    "- `min_count`: This parameter ignores all words with total frequency lower than this. In this case, any word that does not occur at least 20 times across all documents is ignored.\n",
    "\n",
    "- `window`: The maximum distance between the current and predicted word within a sentence. In this case, only words that are within a distance of 2 words from the target word are considered in the context.\n",
    "\n",
    "- `vector_size`: The dimensionality of the word vectors. Here, each word is represented as a 300-dimensional vector.\n",
    "\n",
    "- `sample`: The threshold for configuring which higher-frequency words are randomly downsampled. In this case, words that appear with a frequency greater than 6e-5 are downsampled.\n",
    "\n",
    "- `alpha`: The initial learning rate. Here, the initial learning rate is set to 0.03.\n",
    "\n",
    "- `min_alpha`: Learning rate will linearly drop to `min_alpha` as training progresses. Here, the learning rate drops to 0.0007.\n",
    "\n",
    "- `negative`: If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drawn (usually between 5-20). Here, 20 noise words are drawn.\n",
    "\n",
    "- `workers`: Use these many worker threads to train the model (=faster training with multicore machines). Here, it's set to one less than the total number of cores to leave one core free for other processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1632c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=cores - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68b7c60",
   "metadata": {},
   "source": [
    "# Building the Vocabulary\n",
    "\n",
    "Before we can train our Word2Vec model, we need to build the vocabulary. The vocabulary in this context refers to the set of unique words in our corpus. Each unique word has a unique vector in the Word2Vec model, so building the vocabulary is essentially defining the feature space.\n",
    "\n",
    "The `build_vocab` method in Gensim's Word2Vec expects a sequence of sentences as its input, where each sentence is a list of words. In other words, the input should be a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44746ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.02 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "# Now we can build the vocabulary\n",
    "w2v_model.build_vocab(sentences)\n",
    "\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0869de6",
   "metadata": {},
   "source": [
    "### Train the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe727934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.81 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f55c0a",
   "metadata": {},
   "source": [
    "# Exploring the Model\n",
    "\n",
    "Once the Word2Vec model is trained, we can explore the model in various ways. Here are a few common methods:\n",
    "\n",
    "- **Similarity Queries**: We can use the `most_similar` method to find the words most similar to a given word. For example, `w2v_model.wv.most_similar(\"football\")` would return the words most similar to \"football\" according to the model.\n",
    "\n",
    "- **Odd-One-Out**: We can use the `doesnt_match` method to find the word that doesn't match the others in a list. For example, `w2v_model.wv.doesnt_match([\"football\", \"basketball\", \"apple\"])` would likely return \"apple\".\n",
    "\n",
    "- **Analogy Difference**: We can perform vector arithmetic with the word vectors to find interesting semantic relationships. For example, `w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])` might return \"queen\", completing the analogy \"man is to king as woman is to ___\".\n",
    "\n",
    "- **Word Vector**: We can directly access the vector of a word through the `wv` attribute. For example, `w2v_model.wv[\"football\"]` would return the 300-dimensional vector representation of \"football\".\n",
    "\n",
    "Remember, the quality and usefulness of these operations will depend heavily on the quality of the trained model, which in turn depends on factors like the size and quality of the training data, the choice of model parameters, and the amount of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea08a9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('basketball', 0.613662600517273),\n",
       " ('pro', 0.5756626129150391),\n",
       " ('fantasy', 0.5238478779792786),\n",
       " ('league', 0.5225974917411804),\n",
       " ('groin', 0.5112829208374023),\n",
       " ('hockey', 0.5071981549263),\n",
       " ('stadium', 0.5038345456123352),\n",
       " ('wrestling', 0.4712387025356293),\n",
       " ('player', 0.46910229325294495),\n",
       " ('team', 0.4416936933994293)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4410c303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.doesnt_match([\"football\", \"basketball\", \"apple\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b89ff71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.38972997665405273),\n",
       " ('queen', 0.3561096489429474),\n",
       " ('prom', 0.3003641664981842),\n",
       " ('tale', 0.278092622756958),\n",
       " ('sex', 0.27013933658599854),\n",
       " ('wisdom', 0.2689722776412964),\n",
       " ('di', 0.26756322383880615),\n",
       " ('mr', 0.2590598165988922),\n",
       " ('rumors', 0.2560378313064575),\n",
       " ('prince', 0.25016871094703674)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9757f78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.80672660e-01,  9.09302682e-02,  3.23254138e-01, -1.56710431e-01,\n",
       "       -8.45838934e-02,  2.15427235e-01,  8.66155922e-01,  1.97948162e-02,\n",
       "       -3.43266577e-01, -1.33397549e-01,  2.48938918e-01, -1.92735791e-01,\n",
       "       -4.58437903e-03, -4.63131547e-01, -5.21827117e-02, -2.12247800e-02,\n",
       "       -1.20234919e+00, -1.17120659e+00, -2.66587343e-02,  2.62439460e-01,\n",
       "        6.53793588e-02, -1.04228206e-01, -5.68211257e-01,  6.86273456e-01,\n",
       "       -4.89263505e-01, -9.93103981e-01,  8.27904344e-02,  7.15437829e-01,\n",
       "       -6.70806854e-04,  2.60650516e-01, -3.48504968e-02,  4.38413888e-01,\n",
       "       -3.63642991e-01,  5.02590425e-02,  2.87028760e-01, -1.70110762e-01,\n",
       "       -4.80886772e-02, -8.03541124e-01,  7.53154337e-01, -7.72180855e-01,\n",
       "        3.89634997e-01, -6.07918985e-02, -8.02099466e-01, -2.39952832e-01,\n",
       "       -1.91227496e-01, -2.95303762e-01,  5.77879190e-01,  4.63241667e-01,\n",
       "       -5.74528575e-02,  4.91886377e-01, -4.68041241e-01, -3.79574537e-01,\n",
       "       -6.18054904e-02,  6.13406479e-01, -7.71064699e-01, -6.13301694e-02,\n",
       "        3.19272488e-01,  4.61141616e-01, -1.49549916e-01,  3.55700672e-01,\n",
       "        1.46091446e-01, -1.22397579e-01, -4.96398062e-01,  1.72728091e-01,\n",
       "       -1.18372448e-01, -5.68554163e-01, -5.40397950e-02,  6.23365343e-01,\n",
       "        1.50056377e-01,  3.24043185e-01,  1.30680008e-02, -3.93538743e-01,\n",
       "        1.96597874e-01, -3.77171189e-01, -6.72010303e-01, -4.40946221e-02,\n",
       "        2.47568890e-01, -4.27001387e-01, -4.26810741e-01,  8.11953962e-01,\n",
       "       -1.25957191e+00, -3.31904978e-01,  1.49464116e-01, -2.15477765e-01,\n",
       "        3.94793063e-01,  1.84205800e-01, -2.25242645e-01, -2.91730696e-03,\n",
       "       -5.02221286e-01, -5.75067140e-02,  1.43997774e-01,  4.37669665e-01,\n",
       "        5.02621710e-01,  7.16894686e-01, -4.24777091e-01, -1.78405836e-01,\n",
       "        5.76701462e-01, -3.69349979e-02, -1.08784802e-01,  1.13958433e-01,\n",
       "        9.36677307e-02,  7.51035392e-01,  6.93112761e-02,  8.75648618e-01,\n",
       "        6.95160270e-01, -2.24990085e-01, -6.68311238e-01, -6.85341179e-01,\n",
       "       -1.33833826e-01,  4.66222316e-01,  5.65344870e-01, -1.21700728e+00,\n",
       "        1.16897732e-01, -3.81276518e-01,  1.33206427e-01, -5.42652965e-01,\n",
       "       -2.04648748e-02, -4.50230598e-01, -1.58842690e-02, -4.34025407e-01,\n",
       "        3.10375430e-02, -8.99751782e-01,  2.22832844e-01, -3.35583746e-01,\n",
       "        1.10322833e-01,  6.26700163e-01,  8.39886546e-01,  5.03161401e-02,\n",
       "       -3.13996315e-01,  1.13229133e-01, -7.48228014e-01,  2.15237856e-01,\n",
       "        3.21062416e-01, -2.40286104e-02,  5.16863585e-01, -1.26551405e-01,\n",
       "        9.99500811e-01, -8.00575256e-01,  1.54640436e-01,  7.98306644e-01,\n",
       "        7.82494962e-01, -2.31090367e-01,  4.61347371e-01, -9.23483551e-01,\n",
       "        3.12803954e-01,  5.09170592e-01, -2.12743916e-02,  3.20289247e-02,\n",
       "        4.63197052e-01, -2.94378012e-01, -2.87770808e-01,  1.31262988e-01,\n",
       "        6.34076357e-01, -5.47383666e-01,  1.55027390e-01, -1.19513221e-01,\n",
       "       -3.31973493e-01,  3.44065785e-01, -4.14592594e-01,  7.46776879e-01,\n",
       "       -5.90618253e-01, -3.24670151e-02,  3.19566697e-01,  1.06805325e+00,\n",
       "       -2.91703403e-01,  2.46634051e-01,  8.80687404e-03, -2.02945918e-01,\n",
       "        1.85772315e-01,  2.31569231e-01, -8.72173250e-01,  8.47632051e-01,\n",
       "       -2.37235919e-01,  4.23551351e-03, -1.31735206e+00,  3.02025199e-01,\n",
       "        9.53796208e-01, -6.03286386e-01, -4.02797014e-02, -8.84346589e-02,\n",
       "       -7.16636181e-02, -4.90263760e-01, -1.38833627e-01,  9.17159393e-02,\n",
       "       -2.44564757e-01,  4.98650253e-01, -3.69831800e-01, -3.05510670e-01,\n",
       "        5.86662412e-01, -4.77240592e-01,  9.37634408e-02,  2.41533536e-02,\n",
       "       -8.42188373e-02,  6.84984386e-01,  2.66205817e-01,  4.65857349e-02,\n",
       "        3.68429482e-01,  2.49670416e-01, -7.84665763e-01,  1.54496491e-01,\n",
       "       -3.77646893e-01, -3.17708142e-02,  8.48476529e-01,  2.87560225e-01,\n",
       "       -5.73478118e-02, -1.48499489e-01,  1.21095382e-01, -7.53771067e-01,\n",
       "        3.03800166e-01, -6.62688673e-01,  4.14970279e-01,  6.95283949e-01,\n",
       "        6.29501104e-01, -1.01068437e+00,  6.09988034e-01,  4.92969304e-01,\n",
       "        4.33572948e-01,  3.58081877e-01,  4.20950472e-01, -2.40272120e-01,\n",
       "       -3.80478054e-01, -4.28630680e-01, -4.15661126e-01,  4.24329013e-01,\n",
       "       -1.80964053e-01, -3.32851201e-01, -7.93006539e-01,  2.48371407e-01,\n",
       "       -3.85836065e-01, -1.08908856e+00,  4.80218142e-01,  6.88310564e-02,\n",
       "        1.81217715e-01,  3.72614294e-01,  7.71159768e-01, -5.84770977e-01,\n",
       "        1.25626907e-01, -1.38166532e-01,  4.24934596e-01,  6.63566291e-01,\n",
       "       -3.26133698e-01, -1.68233380e-01, -2.00794414e-01,  1.40554726e-01,\n",
       "        3.31053287e-02, -2.55472690e-01, -1.09245408e+00,  8.30903798e-02,\n",
       "       -2.61327296e-01,  3.84384692e-01,  7.45497465e-01, -1.09009957e+00,\n",
       "        3.37832719e-01, -1.78447723e-01, -1.47673368e-01,  1.23730756e-01,\n",
       "        1.49428740e-01,  1.78210493e-02, -2.84310132e-02,  2.70201057e-01,\n",
       "        2.78893888e-01, -6.54797480e-02, -4.59817082e-01,  2.43698001e-01,\n",
       "        4.22237545e-01, -8.86262178e-01,  6.51172101e-01,  2.21277282e-01,\n",
       "        8.03974807e-01,  7.61249304e-01, -6.96711659e-01,  2.36683458e-01,\n",
       "        5.13342619e-01, -7.73011506e-01,  1.80513382e-01, -7.36592770e-01,\n",
       "       -2.07892612e-01, -4.24125820e-01,  7.92737603e-02, -3.03544670e-01,\n",
       "        8.26898098e-01, -4.46213245e-01, -4.35072541e-01, -2.22238936e-02,\n",
       "        3.81651931e-02, -1.96359232e-01,  1.13469131e-01,  4.54481661e-01,\n",
       "       -1.87871978e-01, -9.35848206e-02,  3.27185512e-01, -2.30186507e-01,\n",
       "        2.92047769e-01, -1.38307482e-01, -6.16469443e-01,  5.35220861e-01,\n",
       "        9.47541296e-01, -5.02992123e-02,  4.65305112e-02,  2.49869227e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"football\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
